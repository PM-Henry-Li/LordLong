#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•å†…å®¹ç”Ÿæˆç¼“å­˜é›†æˆ

ä»»åŠ¡ 9.1.4: æµ‹è¯•ç¼“å­˜é›†æˆ
- æµ‹è¯•ç¼“å­˜å‘½ä¸­
- æµ‹è¯•ç¼“å­˜æœªå‘½ä¸­
- æµ‹è¯•ç¼“å­˜å¤±æ•ˆ

ç›®æ ‡ï¼šæµ‹è¯•è¦†ç›–ç‡ > 70%

æ³¨æ„ï¼šç”±äº _check_cache å’Œ _save_to_cache æ˜¯åµŒå¥—åœ¨ _build_generation_prompt å†…éƒ¨çš„å‡½æ•°ï¼Œ
æˆ‘ä»¬æ— æ³•ç›´æ¥æµ‹è¯•å®ƒä»¬ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šè¿‡æµ‹è¯• generate_content æ–¹æ³•çš„è¡Œä¸ºæ¥é—´æ¥æµ‹è¯•ç¼“å­˜åŠŸèƒ½ã€‚
"""

import pytest
import json
import time
from unittest.mock import Mock, patch, MagicMock
from typing import Dict, Any

from src.content_generator import RedBookContentGenerator
from src.core.config_manager import ConfigManager
from src.core.cache_manager import CacheManager


# ============================================================================
# æµ‹è¯•å›ºä»¶
# ============================================================================


@pytest.fixture
def test_config_with_cache(temp_dir):
    """åˆ›å»ºå¯ç”¨ç¼“å­˜çš„æµ‹è¯•é…ç½®"""
    config_data = {
        "input_file": str(temp_dir / "input.txt"),
        "output_excel": str(temp_dir / "output" / "test.xlsx"),
        "output_image_dir": str(temp_dir / "output" / "images"),
        "openai_api_key": "test-api-key-12345",
        "openai_model": "qwen-plus",
        "openai_base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
        "cache": {
            "enabled": True,
            "max_size": 100,
            "default_ttl": 3600,  # 1å°æ—¶
        },
        "rate_limit": {"openai": {"enable_rate_limit": False}},
    }

    config_file = temp_dir / "config.json"
    with open(config_file, "w", encoding="utf-8") as f:
        json.dump(config_data, f, ensure_ascii=False, indent=2)

    return config_file


@pytest.fixture
def test_config_with_short_ttl(temp_dir):
    """åˆ›å»ºçŸ­TTLçš„æµ‹è¯•é…ç½®ï¼ˆç”¨äºæµ‹è¯•ç¼“å­˜å¤±æ•ˆï¼‰"""
    config_data = {
        "input_file": str(temp_dir / "input.txt"),
        "output_excel": str(temp_dir / "output" / "test.xlsx"),
        "output_image_dir": str(temp_dir / "output" / "images"),
        "openai_api_key": "test-api-key-12345",
        "openai_model": "qwen-plus",
        "openai_base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
        "cache": {
            "enabled": True,
            "max_size": 100,
            "default_ttl": 1,  # 1ç§’ï¼ˆç”¨äºæµ‹è¯•å¤±æ•ˆï¼‰
        },
        "rate_limit": {"openai": {"enable_rate_limit": False}},
    }

    config_file = temp_dir / "config.json"
    with open(config_file, "w", encoding="utf-8") as f:
        json.dump(config_data, f, ensure_ascii=False, indent=2)

    return config_file


@pytest.fixture
def generator_with_cache(test_config_with_cache):
    """åˆ›å»ºå¯ç”¨ç¼“å­˜çš„å†…å®¹ç”Ÿæˆå™¨å®ä¾‹"""
    config_manager = ConfigManager(str(test_config_with_cache))
    return RedBookContentGenerator(config_manager=config_manager)


@pytest.fixture
def generator_with_short_ttl(test_config_with_short_ttl):
    """åˆ›å»ºçŸ­TTLçš„å†…å®¹ç”Ÿæˆå™¨å®ä¾‹"""
    config_manager = ConfigManager(str(test_config_with_short_ttl))
    return RedBookContentGenerator(config_manager=config_manager)


@pytest.fixture
def mock_openai_response():
    """æ¨¡æ‹Ÿ OpenAI API å“åº”"""
    return {
        "titles": [
            "èƒ¡åŒé‡Œçš„è€åŒ—äº¬è®°å¿† ğŸ®",
            "é‚£äº›å¹´ï¼Œæˆ‘ä»¬ä¸€èµ·èµ°è¿‡çš„èƒ¡åŒ",
            "è€åŒ—äº¬èƒ¡åŒï¼šæ—¶å…‰é‡Œçš„æ¸©æš–",
            "èƒ¡åŒæ·±å¤„çš„ç«¥å¹´æ—¶å…‰",
            "å¯»æ‰¾è€åŒ—äº¬çš„å‘³é“",
        ],
        "content": "è®°å¾—å°æ—¶å€™çš„è€åŒ—äº¬èƒ¡åŒå—ï¼Ÿæ¸…æ™¨çš„è±†è…å†å–å£°ä»å··å­æ·±å¤„ä¼ æ¥ï¼Œæ‚ é•¿è€Œäº²åˆ‡ã€‚",
        "tags": "#è€åŒ—äº¬ #èƒ¡åŒæ–‡åŒ– #ç«¥å¹´å›å¿†",
        "image_prompts": [
            {"scene": "èƒ¡åŒæ¸…æ™¨", "prompt": "è€åŒ—äº¬èƒ¡åŒæ¸…æ™¨åœºæ™¯"},
            {"scene": "å­©å­ä»¬ç©è€", "prompt": "èƒ¡åŒé‡Œçš„å­©å­ä»¬åœ¨ç©è€"},
            {"scene": "å››åˆé™¢", "prompt": "ä¼ ç»ŸåŒ—äº¬å››åˆé™¢"},
            {"scene": "é‚»é‡ŒèŠå¤©", "prompt": "é‚»å±…ä»¬åœ¨èƒ¡åŒå£èŠå¤©"},
        ],
        "cover": {"scene": "èƒ¡åŒå…¨æ™¯", "title": "è€åŒ—äº¬èƒ¡åŒè®°å¿†", "prompt": "è€åŒ—äº¬èƒ¡åŒå…¨æ™¯"},
    }


# ============================================================================
# æµ‹è¯• 1: ç¼“å­˜åˆå§‹åŒ–
# ============================================================================


@pytest.mark.unit
def test_cache_initialization_enabled(generator_with_cache):
    """æµ‹è¯•ç¼“å­˜åˆå§‹åŒ– - å¯ç”¨ç¼“å­˜"""
    # éªŒè¯ç¼“å­˜å·²å¯ç”¨
    assert generator_with_cache._cache_enabled is True
    assert generator_with_cache.cache is not None
    assert isinstance(generator_with_cache.cache, CacheManager)


@pytest.mark.unit
def test_cache_initialization_disabled(test_config_with_cache, temp_dir):
    """æµ‹è¯•ç¼“å­˜åˆå§‹åŒ– - ç¦ç”¨ç¼“å­˜"""
    # ä¿®æ”¹é…ç½®ç¦ç”¨ç¼“å­˜
    config_data = {
        "input_file": str(temp_dir / "input.txt"),
        "output_excel": str(temp_dir / "output" / "test.xlsx"),
        "output_image_dir": str(temp_dir / "output" / "images"),
        "openai_api_key": "test-api-key-12345",
        "cache": {"enabled": False},
        "rate_limit": {"openai": {"enable_rate_limit": False}},
    }

    config_file = temp_dir / "config_no_cache.json"
    with open(config_file, "w", encoding="utf-8") as f:
        json.dump(config_data, f, ensure_ascii=False, indent=2)

    config_manager = ConfigManager(str(config_file))
    generator = RedBookContentGenerator(config_manager=config_manager)

    # éªŒè¯ç¼“å­˜è¢«ç¦ç”¨
    assert generator._cache_enabled is False
    assert generator.cache is None


# ============================================================================
# æµ‹è¯• 2: ç¼“å­˜é”®ç”Ÿæˆ
# ============================================================================


@pytest.mark.unit
def test_cache_key_generation_consistency(generator_with_cache):
    """æµ‹è¯•ç¼“å­˜é”®ç”Ÿæˆçš„ä¸€è‡´æ€§"""
    input_text = "è€åŒ—äº¬çš„èƒ¡åŒæ–‡åŒ–"

    key1 = generator_with_cache._generate_cache_key(input_text)
    key2 = generator_with_cache._generate_cache_key(input_text)

    # ç›¸åŒè¾“å…¥åº”è¯¥ç”Ÿæˆç›¸åŒçš„ç¼“å­˜é”®
    assert key1 == key2
    assert key1.startswith("content_gen:")


@pytest.mark.unit
def test_cache_key_generation_uniqueness(generator_with_cache):
    """æµ‹è¯•ç¼“å­˜é”®ç”Ÿæˆçš„å”¯ä¸€æ€§"""
    input_text1 = "è€åŒ—äº¬çš„èƒ¡åŒæ–‡åŒ–"
    input_text2 = "ä¸åŒçš„è¾“å…¥å†…å®¹"

    key1 = generator_with_cache._generate_cache_key(input_text1)
    key2 = generator_with_cache._generate_cache_key(input_text2)

    # ä¸åŒè¾“å…¥åº”è¯¥ç”Ÿæˆä¸åŒçš„ç¼“å­˜é”®
    assert key1 != key2


# ============================================================================
# æµ‹è¯• 3: ç¼“å­˜æœªå‘½ä¸­ï¼ˆç¬¬ä¸€æ¬¡ç”Ÿæˆï¼‰
# ============================================================================


@pytest.mark.unit
def test_cache_miss_first_generation(generator_with_cache, mock_openai_response):
    """æµ‹è¯•ç¼“å­˜æœªå‘½ä¸­ - ç¬¬ä¸€æ¬¡ç”Ÿæˆå†…å®¹"""
    input_text = "è€åŒ—äº¬çš„èƒ¡åŒæ–‡åŒ–"

    # æ¨¡æ‹Ÿ API è°ƒç”¨
    with patch.object(
        generator_with_cache.api_handler, "call_openai_with_evaluation"
    ) as mock_call:
        mock_call.return_value = mock_openai_response

        # ç¬¬ä¸€æ¬¡è°ƒç”¨åº”è¯¥è§¦å‘ API è°ƒç”¨
        result = generator_with_cache.generate_content(input_text)

        # éªŒè¯ç»“æœ
        assert isinstance(result, dict)
        assert "titles" in result
        assert "content" in result

        # éªŒè¯ API è¢«è°ƒç”¨äº†
        assert mock_call.called
        assert mock_call.call_count >= 1


# ============================================================================
# æµ‹è¯• 4: ç¼“å­˜å‘½ä¸­ï¼ˆç¬¬äºŒæ¬¡ç”Ÿæˆï¼‰
# ============================================================================


@pytest.mark.unit
def test_cache_hit_second_generation(generator_with_cache, mock_openai_response):
    """æµ‹è¯•ç¼“å­˜å‘½ä¸­ - ç¬¬äºŒæ¬¡ç”Ÿæˆç›¸åŒå†…å®¹"""
    input_text = "è€åŒ—äº¬çš„èƒ¡åŒæ–‡åŒ–"

    # æ¨¡æ‹Ÿ API è°ƒç”¨
    with patch.object(
        generator_with_cache.api_handler, "call_openai_with_evaluation"
    ) as mock_call:
        mock_call.return_value = mock_openai_response

        # ç¬¬ä¸€æ¬¡è°ƒç”¨ - ç¼“å­˜æœªå‘½ä¸­
        result1 = generator_with_cache.generate_content(input_text)
        first_call_count = mock_call.call_count

        # ç¬¬äºŒæ¬¡è°ƒç”¨ - åº”è¯¥å‘½ä¸­ç¼“å­˜
        result2 = generator_with_cache.generate_content(input_text)
        second_call_count = mock_call.call_count

        # éªŒè¯ç»“æœç›¸åŒ
        assert result1 == result2

        # éªŒè¯ç¬¬äºŒæ¬¡è°ƒç”¨æ²¡æœ‰è§¦å‘ APIï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
        assert second_call_count == first_call_count


@pytest.mark.unit
def test_cache_hit_performance_improvement(generator_with_cache, mock_openai_response):
    """æµ‹è¯•ç¼“å­˜å‘½ä¸­ - æ€§èƒ½æå‡"""
    input_text = "è€åŒ—äº¬çš„èƒ¡åŒæ–‡åŒ–"

    # æ¨¡æ‹Ÿ API è°ƒç”¨ï¼ˆå¸¦å»¶è¿Ÿï¼‰
    def slow_api_call(*args, **kwargs):
        time.sleep(0.1)  # æ¨¡æ‹Ÿ API å»¶è¿Ÿ
        return mock_openai_response

    with patch.object(
        generator_with_cache.api_handler, "call_openai_with_evaluation"
    ) as mock_call:
        mock_call.side_effect = slow_api_call

        # ç¬¬ä¸€æ¬¡è°ƒç”¨ - ç¼“å­˜æœªå‘½ä¸­ï¼ˆæ…¢ï¼‰
        start_time = time.time()
        result1 = generator_with_cache.generate_content(input_text)
        first_duration = time.time() - start_time

        # ç¬¬äºŒæ¬¡è°ƒç”¨ - ç¼“å­˜å‘½ä¸­ï¼ˆå¿«ï¼‰
        start_time = time.time()
        result2 = generator_with_cache.generate_content(input_text)
        second_duration = time.time() - start_time

        # éªŒè¯ç»“æœç›¸åŒ
        assert result1 == result2

        # éªŒè¯ç¬¬äºŒæ¬¡è°ƒç”¨æ›´å¿«ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
        assert second_duration < first_duration
        assert second_duration < 0.05  # ç¼“å­˜å‘½ä¸­åº”è¯¥éå¸¸å¿«


# ============================================================================
# æµ‹è¯• 5: ç¼“å­˜å¤±æ•ˆï¼ˆTTL è¿‡æœŸï¼‰
# ============================================================================


@pytest.mark.unit
def test_cache_expiration_ttl(generator_with_short_ttl, mock_openai_response):
    """æµ‹è¯•ç¼“å­˜å¤±æ•ˆ - TTL è¿‡æœŸ"""
    input_text = "è€åŒ—äº¬çš„èƒ¡åŒæ–‡åŒ–"

    # æ¨¡æ‹Ÿ API è°ƒç”¨
    with patch.object(
        generator_with_short_ttl.api_handler, "call_openai_with_evaluation"
    ) as mock_call:
        mock_call.return_value = mock_openai_response

        # ç¬¬ä¸€æ¬¡è°ƒç”¨ - ç¼“å­˜æœªå‘½ä¸­
        result1 = generator_with_short_ttl.generate_content(input_text)
        first_call_count = mock_call.call_count

        # ç­‰å¾…ç¼“å­˜è¿‡æœŸï¼ˆTTL = 1ç§’ï¼‰
        time.sleep(1.5)

        # ç¬¬äºŒæ¬¡è°ƒç”¨ - ç¼“å­˜å·²è¿‡æœŸï¼Œåº”è¯¥é‡æ–°ç”Ÿæˆ
        result2 = generator_with_short_ttl.generate_content(input_text)
        second_call_count = mock_call.call_count

        # éªŒè¯ç»“æœç›¸åŒï¼ˆå†…å®¹ç›¸åŒï¼‰
        assert result1 == result2

        # éªŒè¯ç¬¬äºŒæ¬¡è°ƒç”¨è§¦å‘äº† APIï¼ˆç¼“å­˜å·²è¿‡æœŸï¼‰
        assert second_call_count > first_call_count


# ============================================================================
# æµ‹è¯• 6: ç¼“å­˜ä¿å­˜ï¼ˆé€šè¿‡ generate_content é—´æ¥æµ‹è¯•ï¼‰
# ============================================================================


@pytest.mark.unit
def test_cache_save_through_generate(generator_with_cache, mock_openai_response):
    """æµ‹è¯•ç¼“å­˜ä¿å­˜ - é€šè¿‡ generate_content é—´æ¥æµ‹è¯•"""
    input_text = "è€åŒ—äº¬çš„èƒ¡åŒæ–‡åŒ–"

    # æ¨¡æ‹Ÿ API è°ƒç”¨
    with patch.object(
        generator_with_cache.api_handler, "call_openai_with_evaluation"
    ) as mock_call:
        mock_call.return_value = mock_openai_response

        # ç¬¬ä¸€æ¬¡è°ƒç”¨ - åº”è¯¥ä¿å­˜åˆ°ç¼“å­˜
        result1 = generator_with_cache.generate_content(input_text)

        # éªŒè¯ç¼“å­˜ä¸­æœ‰æ•°æ®
        cache_key = generator_with_cache._generate_cache_key(input_text)
        cached_result = generator_with_cache.cache.get(cache_key)

        assert cached_result is not None
        assert cached_result == result1


# ============================================================================
# æµ‹è¯• 7: ç¼“å­˜ç»Ÿè®¡
# ============================================================================


@pytest.mark.unit
def test_cache_stats_enabled(generator_with_cache, mock_openai_response):
    """æµ‹è¯•ç¼“å­˜ç»Ÿè®¡ - ç¼“å­˜å¯ç”¨æ—¶"""
    input_text = "è€åŒ—äº¬çš„èƒ¡åŒæ–‡åŒ–"

    # æ¨¡æ‹Ÿ API è°ƒç”¨å¹¶ç”Ÿæˆå†…å®¹ï¼ˆä¼šä¿å­˜åˆ°ç¼“å­˜ï¼‰
    with patch.object(
        generator_with_cache.api_handler, "call_openai_with_evaluation"
    ) as mock_call:
        mock_call.return_value = mock_openai_response
        generator_with_cache.generate_content(input_text)

    # è·å–ç¼“å­˜ç»Ÿè®¡
    stats = generator_with_cache.get_cache_stats()

    # éªŒè¯ç»Ÿè®¡ä¿¡æ¯
    assert stats is not None
    assert isinstance(stats, dict)
    assert "size" in stats
    assert "max_size" in stats
    assert "hits" in stats
    assert "misses" in stats
    assert "hit_rate" in stats


@pytest.mark.unit
def test_cache_stats_disabled(test_config_with_cache, temp_dir):
    """æµ‹è¯•ç¼“å­˜ç»Ÿè®¡ - ç¼“å­˜ç¦ç”¨æ—¶"""
    # åˆ›å»ºç¦ç”¨ç¼“å­˜çš„ç”Ÿæˆå™¨
    config_data = {
        "input_file": str(temp_dir / "input.txt"),
        "output_excel": str(temp_dir / "output" / "test.xlsx"),
        "output_image_dir": str(temp_dir / "output" / "images"),
        "openai_api_key": "test-api-key-12345",
        "cache": {"enabled": False},
        "rate_limit": {"openai": {"enable_rate_limit": False}},
    }

    config_file = temp_dir / "config_no_cache.json"
    with open(config_file, "w", encoding="utf-8") as f:
        json.dump(config_data, f, ensure_ascii=False, indent=2)

    config_manager = ConfigManager(str(config_file))
    generator = RedBookContentGenerator(config_manager=config_manager)

    # è·å–ç¼“å­˜ç»Ÿè®¡ï¼ˆåº”è¯¥è¿”å› Noneï¼‰
    stats = generator.get_cache_stats()

    assert stats is None


# ============================================================================
# æµ‹è¯• 8: ç¼“å­˜æ¸…ç©º
# ============================================================================


@pytest.mark.unit
def test_cache_clear(generator_with_cache, mock_openai_response):
    """æµ‹è¯•ç¼“å­˜æ¸…ç©º"""
    input_text1 = "è€åŒ—äº¬çš„èƒ¡åŒæ–‡åŒ–"
    input_text2 = "ä¸åŒçš„è¾“å…¥å†…å®¹"

    # æ¨¡æ‹Ÿ API è°ƒç”¨å¹¶ç”Ÿæˆå¤šä¸ªå†…å®¹
    with patch.object(
        generator_with_cache.api_handler, "call_openai_with_evaluation"
    ) as mock_call:
        mock_call.return_value = mock_openai_response

        # ç”Ÿæˆä¸¤ä¸ªå†…å®¹ï¼ˆä¼šä¿å­˜åˆ°ç¼“å­˜ï¼‰
        generator_with_cache.generate_content(input_text1)
        generator_with_cache.generate_content(input_text2)

    # éªŒè¯ç¼“å­˜å­˜åœ¨
    cache_key1 = generator_with_cache._generate_cache_key(input_text1)
    cache_key2 = generator_with_cache._generate_cache_key(input_text2)
    assert generator_with_cache.cache.get(cache_key1) is not None
    assert generator_with_cache.cache.get(cache_key2) is not None

    # æ¸…ç©ºç¼“å­˜
    generator_with_cache.clear_cache()

    # éªŒè¯ç¼“å­˜å·²æ¸…ç©º
    assert generator_with_cache.cache.get(cache_key1) is None
    assert generator_with_cache.cache.get(cache_key2) is None


@pytest.mark.unit
def test_cache_clear_disabled(test_config_with_cache, temp_dir):
    """æµ‹è¯•ç¼“å­˜æ¸…ç©º - ç¼“å­˜ç¦ç”¨æ—¶"""
    # åˆ›å»ºç¦ç”¨ç¼“å­˜çš„ç”Ÿæˆå™¨
    config_data = {
        "input_file": str(temp_dir / "input.txt"),
        "output_excel": str(temp_dir / "output" / "test.xlsx"),
        "output_image_dir": str(temp_dir / "output" / "images"),
        "openai_api_key": "test-api-key-12345",
        "cache": {"enabled": False},
        "rate_limit": {"openai": {"enable_rate_limit": False}},
    }

    config_file = temp_dir / "config_no_cache.json"
    with open(config_file, "w", encoding="utf-8") as f:
        json.dump(config_data, f, ensure_ascii=False, indent=2)

    config_manager = ConfigManager(str(config_file))
    generator = RedBookContentGenerator(config_manager=config_manager)

    # æ¸…ç©ºç¼“å­˜ï¼ˆä¸åº”è¯¥æŠ›å‡ºå¼‚å¸¸ï¼‰
    generator.clear_cache()


# ============================================================================
# æµ‹è¯• 9: ä¸åŒè¾“å…¥çš„ç¼“å­˜éš”ç¦»
# ============================================================================


@pytest.mark.unit
def test_cache_isolation_different_inputs(generator_with_cache, mock_openai_response):
    """æµ‹è¯•ä¸åŒè¾“å…¥çš„ç¼“å­˜éš”ç¦»"""
    input_text1 = "è€åŒ—äº¬çš„èƒ¡åŒæ–‡åŒ–"
    input_text2 = "ä¸åŒçš„è¾“å…¥å†…å®¹"

    # ä¸ºä¸åŒè¾“å…¥åˆ›å»ºä¸åŒçš„å“åº”
    response1 = mock_openai_response.copy()
    response1["content"] = "ç¬¬ä¸€ä¸ªè¾“å…¥çš„å†…å®¹"

    response2 = mock_openai_response.copy()
    response2["content"] = "ç¬¬äºŒä¸ªè¾“å…¥çš„å†…å®¹"

    # æ¨¡æ‹Ÿ API è°ƒç”¨
    with patch.object(
        generator_with_cache.api_handler, "call_openai_with_evaluation"
    ) as mock_call:
        # ç¬¬ä¸€æ¬¡è°ƒç”¨è¿”å› response1
        mock_call.return_value = response1
        result1 = generator_with_cache.generate_content(input_text1)

        # ç¬¬äºŒæ¬¡è°ƒç”¨è¿”å› response2
        mock_call.return_value = response2
        result2 = generator_with_cache.generate_content(input_text2)

    # éªŒè¯ç¼“å­˜éš”ç¦»
    assert result1 is not None
    assert result2 is not None
    assert result1["content"] == "ç¬¬ä¸€ä¸ªè¾“å…¥çš„å†…å®¹"
    assert result2["content"] == "ç¬¬äºŒä¸ªè¾“å…¥çš„å†…å®¹"
    assert result1 != result2


# ============================================================================
# æµ‹è¯• 10: ç¼“å­˜ä¸ API è°ƒç”¨çš„é›†æˆ
# ============================================================================


@pytest.mark.unit
def test_cache_integration_with_api_calls(generator_with_cache, mock_openai_response):
    """æµ‹è¯•ç¼“å­˜ä¸ API è°ƒç”¨çš„é›†æˆ"""
    input_text = "è€åŒ—äº¬çš„èƒ¡åŒæ–‡åŒ–"

    # æ¨¡æ‹Ÿ API è°ƒç”¨
    with patch.object(
        generator_with_cache.api_handler, "call_openai_with_evaluation"
    ) as mock_call:
        mock_call.return_value = mock_openai_response

        # ç¬¬ä¸€æ¬¡è°ƒç”¨ - åº”è¯¥è§¦å‘ API è°ƒç”¨å¹¶ä¿å­˜åˆ°ç¼“å­˜
        result1 = generator_with_cache.generate_content(input_text)
        assert mock_call.call_count >= 1

        # ç¬¬äºŒæ¬¡è°ƒç”¨ - åº”è¯¥ä»ç¼“å­˜è¯»å–ï¼Œä¸è§¦å‘ API è°ƒç”¨
        first_call_count = mock_call.call_count
        result2 = generator_with_cache.generate_content(input_text)
        assert mock_call.call_count == first_call_count  # è°ƒç”¨æ¬¡æ•°ä¸å˜

        # ç¬¬ä¸‰æ¬¡è°ƒç”¨ - ä»ç„¶ä»ç¼“å­˜è¯»å–
        result3 = generator_with_cache.generate_content(input_text)
        assert mock_call.call_count == first_call_count  # è°ƒç”¨æ¬¡æ•°ä¸å˜

        # éªŒè¯æ‰€æœ‰ç»“æœç›¸åŒ
        assert result1 == result2 == result3


@pytest.mark.unit
def test_cache_integration_multiple_inputs(generator_with_cache, mock_openai_response):
    """æµ‹è¯•ç¼“å­˜ä¸å¤šä¸ªè¾“å…¥çš„é›†æˆ"""
    inputs = [
        "è€åŒ—äº¬çš„èƒ¡åŒæ–‡åŒ–",
        "å››åˆé™¢çš„å»ºç­‘ç‰¹è‰²",
        "ä¼ ç»ŸåŒ—äº¬å°åƒ",
    ]

    # æ¨¡æ‹Ÿ API è°ƒç”¨
    with patch.object(
        generator_with_cache.api_handler, "call_openai_with_evaluation"
    ) as mock_call:
        mock_call.return_value = mock_openai_response

        # ç¬¬ä¸€è½®ï¼šæ‰€æœ‰è¾“å…¥éƒ½åº”è¯¥è§¦å‘ API è°ƒç”¨
        results1 = []
        for input_text in inputs:
            result = generator_with_cache.generate_content(input_text)
            results1.append(result)

        first_round_calls = mock_call.call_count

        # ç¬¬äºŒè½®ï¼šæ‰€æœ‰è¾“å…¥éƒ½åº”è¯¥ä»ç¼“å­˜è¯»å–
        results2 = []
        for input_text in inputs:
            result = generator_with_cache.generate_content(input_text)
            results2.append(result)

        second_round_calls = mock_call.call_count

        # éªŒè¯ç¬¬äºŒè½®æ²¡æœ‰è§¦å‘æ–°çš„ API è°ƒç”¨
        assert second_round_calls == first_round_calls

        # éªŒè¯ç»“æœç›¸åŒ
        for i in range(len(inputs)):
            assert results1[i] == results2[i]


if __name__ == "__main__":
    pytest.main([
        __file__,
        "-v",
        "--cov=src.content_generator",
        "--cov=src.core.cache_manager",
        "--cov-report=term-missing",
    ])
